{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Gaussian Variational Inference (MGVI), Julia and AutoDiff\n",
    "\n",
    "## Background\n",
    "\n",
    "### MGVI\n",
    "\n",
    "* Bayesian inference scheme by Jakob Knollmüller\n",
    "* Variational approximation of true posterior with a Gaussian in standardized coordinates\n",
    "* Minimizes Kullback-Leibler divergence between true posterior and approximation\n",
    "* Appoximates covariance structures via samples drawn according to the Fisher metric\n",
    "\n",
    "### Julia\n",
    "\n",
    "* High-performance programming language\n",
    "* Just-in-time (JIT) compiled\n",
    "* Multiple dispatch\n",
    "* Native AutoDiff\n",
    "\n",
    "### AutoDiff\n",
    "\n",
    "* \"Differentiation is mechanics, integration is art.\"\n",
    "* Forward mode (jacobian-vector-product, pushforward)\n",
    "    * Memory scales independently from the depth of the computational graph\n",
    "    * Very efficient for \"tall\" jacobian matrices\n",
    "* Reverse mode (vector-jacobian-product, pullback)\n",
    "    * Memory scales with the depth of the computational graph\n",
    "    * Very efficient for \"wide\" jacobain matrices (e.g. gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imlementing MGVI in Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplary problem\n",
    "\n",
    "#### Problem definition\n",
    "\n",
    "Let the data generating equation be\n",
    "$$d = R s(\\xi) + n $$\n",
    "with data $d$, signal model $s$, response $R$, Gaussian noise $n$ and parameters $\\xi$.\n",
    "\n",
    "The hierarchical signal model with standardized priors for $\\xi$ is\n",
    "$$ s(\\xi) = \\exp{\\left[ \\mathrm{HT}^{-1} \\circ P \\circ \\xi \\right]} $$\n",
    "with harmonic transform operator $\\mathrm{HT}$ and amplitude operator $P$.\n",
    "The signal model contains both non-linearieties as well global linear functions.\n",
    "\n",
    "Then the negative log-likelihood respectively information hamiltonian of the likelihood reads\n",
    "$$\\mathcal{H}(d|\\xi) = \\mathrm{nll}(\\xi) = \\frac{1}{2} (d - Rs(\\xi))^\\dagger N^{-1} (d - Rs(\\xi))$$\n",
    "with $N$ the noise covariance.\n",
    "\n",
    "Thus, the overall potential, i.e. joint information hamiltonina of data and signal is\n",
    "$$\\mathcal{H}(d,\\xi) = \\mathrm{potential}(\\xi) = \\frac{1}{2} (d - Rs(\\xi))^\\dagger N^{-1} (d - Rs(\\xi)) + \\frac{1}{2} \\xi^\\dagger \\xi.$$\n",
    "\n",
    "#### Solution with MGVI\n",
    "\n",
    "1. First, draw an initially random starting position $\\bar{\\xi}$.\n",
    "1. Construct the Fisher information metric\n",
    "$$\\mathrm{Fisher}(\\xi) = J_\\bar{\\xi}^\\dagger N^{-1} J_\\bar{\\xi} + \\mathbb{1}$$\n",
    "with metric $M = N^{-1}$ and $J_\\bar{\\xi}$ the jacobian of the signal response at $\\bar{\\xi}$.\n",
    "1. Draw samples $\\Delta\\xi_\\cdot$ with the covariance structure defined by the $\\mathrm{Fisher}$ metric.\n",
    "1. Calculate the approximate Kullback-Leiblach divergence\n",
    "$$\\mathcal{D}_\\mathrm{KL}(\\xi) = {\\left\\langle \\mathcal{H}(d,\\xi) \\right\\rangle}_{\\mathcal{G}(\\xi-\\bar{\\xi},\\mathrm{Fisher}(\\bar{\\xi}))} = \\frac{1}{\\text{#samples}} \\sum_{i=0}^{\\text{#samples}} \\mathcal{H}(d, \\xi + \\Delta\\xi_i).$$\n",
    "1. Minimize the KL via its natural gradient\n",
    "\n",
    "Thereby MGVI is a perfect example of why forward ($J_\\bar{\\xi}$) and reverse mode ($J_\\bar{\\xi}^\\dagger$) differentiation is extremely helpful in a programming language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IterativeSolvers: cg\n",
    "import Random: randn,seed!\n",
    "import ForwardDiff\n",
    "import FFTW: plan_r2r, DHT\n",
    "import Base: *, +, ∘, adjoint\n",
    "using ForwardDiff  # Rudimentary for autodiff\n",
    "using Zygote  # Advanced reverse autodiff\n",
    "using LinearAlgebra\n",
    "using LinearMaps  # Treate (linear) functions as if they were matrices\n",
    "using Statistics: mean\n",
    "using Plots\n",
    "\n",
    "using Optim\n",
    "\n",
    "\n",
    "VecOrNum = Union{Number,Vector{<:Number}}\n",
    "\n",
    "seed!(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = (1024)\n",
    "k = [i < dims / 2 ? i :  dims-i for i = 0:dims-1]\n",
    "\n",
    "# Define the harmonic transform operator as a matrix-like object\n",
    "ht = plan_r2r(zeros(dims), DHT)\n",
    "# Unfortunately neither Zygote nor ForwardDiff support planned Hartley\n",
    "# transformations. While Zygote does not support AbstractFFTs.ScaledPlan,\n",
    "# ForwardDiff does not overload the appropriate methods from AbstractFFTs.\n",
    "function *(trafo::typeof(ht), u::Vector{ForwardDiff.Dual{T,V,P}}) where {T,V,P}\n",
    "    # Unpack AoS -> SoA\n",
    "    vs = ForwardDiff.value.(u)\n",
    "    ps = mapreduce(ForwardDiff.partials, vcat, u)\n",
    "    # Actual computation\n",
    "    val = trafo * vs\n",
    "    jvp = trafo * ps\n",
    "    # Pack SoA -> AoS (depending on jvp, might need `eachrow`)\n",
    "    return map((v, p) -> ForwardDiff.Dual{T}(v, p...), val, jvp)\n",
    "end\n",
    "Zygote.@adjoint function *(trafo::typeof(inv(ht)), xs::T) where T\n",
    "    return trafo * xs, Δ -> (nothing, trafo * Δ)\n",
    "end\n",
    "Zygote.@adjoint function inv(trafo::typeof(ht))\n",
    "    inv_t = inv(trafo)\n",
    "    return inv_t, function (Δ)\n",
    "        adj_inv_t = adjoint(inv_t)\n",
    "        return (- adj_inv_t * Δ * adj_inv_t, )\n",
    "    end\n",
    "end\n",
    "\n",
    "# ξ := latent variables\n",
    "ξ_truth = randn(dims)\n",
    "\n",
    "loglogslope = 2.0 + 0.5 * randn()\n",
    "P = @. 50 / (k^loglogslope + 1)\n",
    "function correlated_field(ξ::V) where V<:VecOrNum\n",
    "    return inv(ht) * (P .* ξ)\n",
    "end\n",
    "function signal(ξ::V) where V<:VecOrNum\n",
    "    return exp.(correlated_field(ξ))\n",
    "end\n",
    "\n",
    "N = Diagonal(0.01^2 * ones(dims))\n",
    "R = ones(dims)\n",
    "#R[100:200] .= 0\n",
    "R = Diagonal(R)\n",
    "\n",
    "function signal_response(ξ::V) where V<:VecOrNum\n",
    "    return R * signal(ξ)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic signal and data\n",
    "ss = signal(ξ_truth)\n",
    "d = R * ss .+ R * sqrt(N) * randn(dims)\n",
    "plot(ss, color=:red, label=\"ground truth\", linewidt=5)\n",
    "plot!(d, seriestype=:scatter, marker=:x, color=:black)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@doc \"\"\"Return a mapping function to translate a vector at a given\n",
    "vector-valued position to a combined vector of dual numbers\"\"\"\n",
    "function to_dual_at(ξ::V) where V<:VecOrNum\n",
    "    return function to_dual(δ::V)\n",
    "        return map((v, p) -> ForwardDiff.Dual(v, p...), ξ, δ)\n",
    "    end\n",
    "end\n",
    "\n",
    "@doc \"\"\"Retrieve the jacobian of f at xi as implicit matrix\"\"\"\n",
    "function jacobian(f::F, ξ::V) where {F<:Function, V<:VecOrNum}\n",
    "    to_dual = to_dual_at(ξ)\n",
    "    # HERE is where the magic happens!\n",
    "    jvp(δ::V) = mapreduce(ForwardDiff.partials, vcat, f(to_dual(δ)))\n",
    "\n",
    "    function vjp(δ::T) where T<:VecOrNum\n",
    "        # HERE is more magic!\n",
    "        return first(Zygote.pullback(f, ξ)[2](δ))\n",
    "    end\n",
    "\n",
    "    return LinearMap{eltype(ξ)}(jvp, vjp, first(size(ξ)))\n",
    "end\n",
    "\n",
    "inv_noise_cov = inv(N)\n",
    "\n",
    "function nll(ξ::L) where L\n",
    "    res = d .- signal_response(ξ)\n",
    "    return 0.5 * transpose(res) * inv_noise_cov * res\n",
    "end\n",
    "\n",
    "ham(ξ::L where L) = nll(ξ) + 0.5 * (ξ ⋅ ξ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function covariance_sample(cov_inv::T, jac::N, metric::M) where {\n",
    "    T<:Union{AbstractMatrix,LinearMap{E}},\n",
    "    N<:Union{AbstractMatrix,LinearMap{E}},\n",
    "    M<:Union{AbstractMatrix,LinearMap{E}}\n",
    "} where E\n",
    "    ξ_new::Vector{E} = randn(first(size(cov_inv)))\n",
    "    d_new::Vector{E} = jac * ξ_new .+ sqrt(inv(metric)) * randn(dims)\n",
    "    j_new::Vector{E} = adjoint(jac) * metric * d_new\n",
    "    m_new::Vector{E} = cg(cov_inv, j_new, log=true)[1]\n",
    "    return ξ_new .- m_new\n",
    "end\n",
    "\n",
    "function metric_gaussian_kl!(\n",
    "    pos::P,\n",
    "    n_samples::C;\n",
    "    mirror_samples::Bool=false,\n",
    "    n_grad_steps::C=10,\n",
    "    nat_grad_scl::F=1.0\n",
    ") where {T, P, C<:Int, F<:Number}\n",
    "    jac = jacobian(signal_response, pos)\n",
    "    fisher = adjoint(jac) * inv_noise_cov * jac + I\n",
    "\n",
    "    samples = [covariance_sample(fisher, jac, inv_noise_cov) for i = 1 : n_samples]\n",
    "    samples = mirror_samples ? vcat(samples, -samples) : samples\n",
    "    \n",
    "    kl(ξ::P) = reduce(+, ham(ξ + s) for s in samples) / length(samples)\n",
    "\n",
    "    # Take the metric of the KL itself as curvature\n",
    "    nll_fisher_by_s = mapreduce(+, samples) do s\n",
    "        jac_s = jacobian(signal_response, pos + s)\n",
    "        return adjoint(jac_s) * inv_noise_cov * jac_s\n",
    "    end\n",
    "    avg_fisher = nll_fisher_by_s / length(samples) + I\n",
    "\n",
    "    for _ in 1 : n_grad_steps\n",
    "        grad::P = first(gradient(kl, pos))\n",
    "        Δξ = cg(avg_fisher, grad, log=true)[1]\n",
    "        pos .-= nat_grad_scl * Δξ\n",
    "    end\n",
    "    return pos, samples\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "init_pos = 0.1 * randn(dims)\n",
    "\n",
    "pos = deepcopy(init_pos)\n",
    "n_samples = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos, samples = metric_gaussian_kl!(pos, n_samples; mirror_samples=true, n_grad_steps=15, nat_grad_scl=0.1)\n",
    "plot!(signal(pos), label=\"MGVI it. 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos, samples = metric_gaussian_kl!(pos, n_samples; mirror_samples=true, n_grad_steps=15, nat_grad_scl=0.5)\n",
    "for (i, s) in enumerate(samples)\n",
    "    plot!(signal(pos + s), label=\"Post. Sample \" * string(i), color=:gray)\n",
    "end\n",
    "plot!(signal(pos), label=\"Post. mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Julia\n",
    "\n",
    "* No error prone self-built Jacobians thanks to AutoDiff (here, Zygote and ForwardDiff)\n",
    "* Potentially faster and deployable to a wider infrastructure e.g. GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to do next\n",
    "\n",
    "### Minor\n",
    "\n",
    "* Introduce a line search algorithm\n",
    "* Advance Zygote's work on `pushforward`, i.e. forward mode differentiation for vector-valued parameters thereby enabling true GPU and XLA support\n",
    "\n",
    "### Major\n",
    "\n",
    "* Handling more than one parameter vector\n",
    "    * Ideally it should treat the parameters as a dictionary of vectors\n",
    "    * Parameter dictionaries require major changes and re-inventing LinearMap\n",
    "* Wrap code for use in python and benchmark against the implementation in Numerical Information Field Theory (NIFTy)\n",
    "* Integrate into Bayesian Analysis Toolkit (BAT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
